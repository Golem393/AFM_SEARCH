{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975263db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import json\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from clip_model import CLIPModel\n",
    "from keyword_extractor import KeyWordExtractor\n",
    "from pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9270235",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up pipeline\n",
    "device = 'mps'\n",
    "embedding_database_path = 'tests/flickr8k/img_embeddings.h5'\n",
    "clipmodel = CLIPModel(device)\n",
    "keywordextractor = KeyWordExtractor('kw-extractor-tokenizer', 'kw-extactor-model')\n",
    "pipeline = Pipeline(clipmodel,keywordextractor, embedding_database_path, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48551283",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some matches\n",
    "matches = pipeline.run(\"dogs playing in a park\", True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf304ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches(selected_imgs):\n",
    "    n_images = len(selected_imgs)\n",
    "    cols = 4  # Adjust this for layout\n",
    "    rows = (n_images + cols - 1) // cols  # Round up division\n",
    "\n",
    "    plt.figure(figsize=(cols * 4, rows * 4))\n",
    "\n",
    "    for i, path in enumerate(selected_imgs):\n",
    "        img = Image.open(path).convert(\"RGB\")\n",
    "        plt.subplot(rows, cols, i + 1)\n",
    "        plt.imshow(img)\n",
    "        plt.title(path.split(\"/\")[-1])  # Show filename as title\n",
    "        plt.axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "plot_matches(matches[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d6852e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall@k benchmark\n",
    "def benchmark_rk(captions_file):\n",
    "    \n",
    "    with open(captions_file, 'r', encoding='utf-8') as file:\n",
    "        captions_dict = json.load(file)\n",
    "\n",
    "    r1, r5, r10 = 0, 0, 0    \n",
    "    \n",
    "    imgs = list(captions_dict.keys())\n",
    "    for i, img in enumerate(imgs):\n",
    "        caption = captions_dict[img][0] # get the first caption of the image\n",
    "\n",
    "        matches = pipeline.run(caption, True)[:10]\n",
    "\n",
    "        img = os.path.join('/Users/benjaminkasper/Documents/Uni/RCI/Module/in2106/dataset/Flicker8k_Dataset/', img)\n",
    "        \n",
    "        # R @ 1\n",
    "        r1 += img in matches[:1]\n",
    "\n",
    "        # R @ 5\n",
    "        r5 += img in matches[:5]\n",
    "\n",
    "        # R @ 10\n",
    "        r10 += img in matches[:10]\n",
    "\n",
    "        print(i, r1, r5, r10)\n",
    "\n",
    "    print(r1/len(imgs), r5/len(imgs), r10/len(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967e21f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# semantic benchmark\n",
    "def benchmark_semantic(captions_file):\n",
    "    \n",
    "    with open(captions_file, 'r', encoding='utf-8') as file:\n",
    "        captions_dict = json.load(file)\n",
    "\n",
    "    sum_similarity = 0   \n",
    "    \n",
    "    imgs = list(captions_dict.keys())\n",
    "    for i, img in enumerate(imgs): # iterate through all images in the dataset\n",
    "\n",
    "        caption = captions_dict[img][0] # get the first caption of the image\n",
    "\n",
    "        matches = pipeline.run(caption, True)[:1] # get top 10 matches of the query\n",
    "\n",
    "        img = os.path.join('/Users/benjaminkasper/Documents/Uni/RCI/Module/in2106/dataset/Flicker8k_Dataset/', img)\n",
    "        \n",
    "        matched_image_emb = clipmodel.get_img_embedding(matches[0]) # get top matched images and compute embedding\n",
    "        queried_image_emb = clipmodel.get_img_embedding(img) # get original image and it's embedding\n",
    "\n",
    "        similarity = np.inner(matched_image_emb, queried_image_emb) # compute semantic similarity: retrieved image is original image\n",
    "        \n",
    "        sum_similarity += similarity[0,0]\n",
    "\n",
    "        print(i, sum_similarity)\n",
    "\n",
    "    print(sum_similarity/(imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6f9900",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_semantic('tests/flickr8k/flickr8k_captions.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "in2106",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
